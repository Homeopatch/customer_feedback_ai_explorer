import os
from typing import List
import threading
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class LLMService:
    _instance = None
    _lock = threading.Lock()
    
    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = LLMService()
            return cls._instance
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = "gpt-3.5-turbo"  # Default model
    
    def generate_summary(self, query: str, feedback_texts: List[str]) -> str:
        """
        Generate a summary or answer based on the query and retrieved feedback texts.
        
        Args:
            query: The user's query
            feedback_texts: List of relevant feedback texts
            
        Returns:
            A summary or answer generated by the LLM
        """
        # Prepare the prompt
        prompt = f"""
        Based on the following customer feedback, please provide a concise and insightful answer to this query:
        
        QUERY: {query}
        
        CUSTOMER FEEDBACK:
        {self._format_feedback(feedback_texts)}
        
        Please analyze these feedback entries and provide:
        1. A direct answer to the query
        2. Key insights from the feedback
        3. Any notable patterns or trends
        
        Keep your response concise and focused on the most relevant information.
        """
        
        # Call the OpenAI API
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are an AI assistant that analyzes customer feedback and provides helpful insights."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.5
        )
        
        return response.choices[0].message.content
    
    def _format_feedback(self, feedback_texts: List[str]) -> str:
        """Format the feedback texts for inclusion in the prompt."""
        formatted = ""
        for i, text in enumerate(feedback_texts, 1):
            # Truncate very long feedback to avoid token limits
            if len(text) > 500:
                text = text[:497] + "..."
            formatted += f"{i}. {text}\n\n"
        return formatted